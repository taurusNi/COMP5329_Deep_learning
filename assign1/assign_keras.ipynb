{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Assignment1 Neural Network model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Packages####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "from keras.optimizers import SGD\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from keras.layers.normalization import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorized_label(label):\n",
    "    tmp = np.zeros((10, 1))\n",
    "    tmp[label] = 1.0\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Dataset ####\n",
    "Show the first 10 instances and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 10 instances: \n",
      "[[ -1.26502938e+02   1.63243234e+03  -1.20922145e+03   2.48416923e+02\n",
      "   -6.02124110e-01  -4.04695491e+02  -9.31297310e+01   2.01038160e+02\n",
      "   -3.29514187e+01  -2.77401504e+01   1.50024716e+02  -6.64644771e+01\n",
      "    5.27323547e+01  -1.34801560e+02   1.02081727e+02  -5.85367859e+01\n",
      "   -1.18862042e+02  -1.96611265e+02   1.87822261e+02  -4.35857526e+02\n",
      "   -1.33405657e+02   1.71629407e+02   1.11454164e+02  -2.66110366e+01\n",
      "   -2.06630998e+02   6.95156022e+01  -1.03421989e+02  -1.16215683e+02\n",
      "   -1.16454278e+02   9.40007318e+01  -9.20309210e+01  -3.16710960e+02\n",
      "   -6.25421850e+01  -7.21027152e+01  -3.22370101e+01  -5.43615649e+01\n",
      "    5.58975643e+01  -1.36786583e+02   3.03174619e+01  -1.33945222e+02\n",
      "   -6.15853235e+01  -7.22445118e+01   8.01202173e+01  -1.05298042e+02\n",
      "   -1.13903920e+02  -1.50105891e+02   1.41943854e+02  -4.07795009e+01\n",
      "   -6.84617171e+01   1.11272701e+01  -8.97342715e+01  -3.37421200e+01\n",
      "   -5.39188539e+01  -7.54320096e+01  -7.08386988e+00   1.14355965e+02\n",
      "    4.23271952e+01  -1.54178455e+01   3.43271026e+01  -7.25806413e+01\n",
      "   -3.31648260e+01  -5.22980186e+01  -1.51820109e+01   2.01135004e+01\n",
      "   -6.37224940e+01  -1.14808242e+02   9.77339692e+01  -1.39413015e+02\n",
      "   -5.27778363e+01  -6.36569555e+01   1.82622209e+02  -1.05555894e+02\n",
      "   -3.16043509e+01   1.07345244e+02  -5.01711576e+01  -9.96482632e+01\n",
      "    1.22805636e+02   9.99936990e+01  -4.15406019e+01   5.25083275e+01\n",
      "   -5.00286007e+01   6.14499228e+01  -4.12062987e+01   1.25866753e+02\n",
      "    5.20246671e+01   4.80643973e+01   9.34091500e+01   2.46486335e+01\n",
      "   -7.83340754e+01  -6.62743264e+01  -9.64675811e+01  -1.52810717e+02\n",
      "   -1.16093575e+02  -1.12003886e+02  -8.15142665e+00  -7.31680887e+01\n",
      "    2.97605299e+01   3.35609563e+01  -3.54586258e+01   1.76331332e+01\n",
      "   -5.94037470e+01   9.73550507e+01   3.30546501e+01   2.13296463e+01\n",
      "   -1.23900695e+02  -8.92897554e+00   7.19954921e+01   4.73545677e+00\n",
      "    1.91236482e+00  -3.93153380e+01  -2.27519068e+01  -2.82922512e+01\n",
      "   -9.91396205e+00  -1.23982484e+01  -4.43181823e+00   4.06940775e+01\n",
      "   -1.21561277e+01   3.43056582e+01  -5.36241629e+01   5.10101205e+01\n",
      "   -3.80998294e+01   3.46569134e+01  -7.88282944e+00  -8.48258361e+00\n",
      "   -3.43749107e+01   1.11837600e+02   3.57044951e+01  -6.63192504e-01]\n",
      " [  1.40756479e+03  -4.51681446e+02  -2.59769757e+02   3.68518685e+02\n",
      "    2.14447195e+02   1.26852745e+03  -1.44983974e+02  -2.28151550e+02\n",
      "   -1.26746831e+02  -2.25202768e+02   1.08543338e+01   1.92709827e+02\n",
      "   -2.58300727e+02  -2.52569072e+02   3.67779036e+01  -1.23669749e+02\n",
      "   -2.73932774e+02   3.67922144e+02   1.76938790e+02   1.56012435e+02\n",
      "   -1.74441280e+00   9.72579462e+01  -1.01810400e+02   9.62943617e+01\n",
      "   -2.04209860e+02  -2.19244222e+02  -3.42074606e+01  -3.03692616e+01\n",
      "    2.13005628e+02  -1.66362376e+02  -2.42714704e+00   8.11100396e+01\n",
      "    5.14214588e+01  -1.44139932e+02   1.19710868e+02   5.78926264e+01\n",
      "    1.08630937e+02  -1.71279886e+01  -2.42315337e+01  -5.41573243e+01\n",
      "   -1.41294140e+02   8.59866920e+01   5.02068551e+01   1.13020936e+02\n",
      "    6.74480968e+01   9.95793830e+01   2.33017668e+01  -5.00099521e+01\n",
      "    3.99321352e+01  -2.00558044e+01   1.14789159e+02  -7.61254183e+01\n",
      "    8.91291944e+01   1.51413064e+01   1.92314580e+01  -1.10652006e+02\n",
      "    5.71568009e+01  -6.36999413e+01  -4.09778590e+01  -3.03452185e+01\n",
      "   -1.40253879e+02   4.36624729e+01  -1.66704252e+02   1.95551296e+01\n",
      "   -5.94183417e+01   1.46790520e+02  -1.38202317e+01  -4.29463510e+00\n",
      "   -6.62791075e+01  -3.55197978e+01   6.94766936e+01   5.93165465e+01\n",
      "   -1.59546290e+00  -4.35120504e+01   1.37104354e+01   1.03904845e+01\n",
      "   -1.37470727e+01  -1.23351432e+02  -4.86590448e+01  -1.34076487e+02\n",
      "   -1.77698493e+02  -5.32448932e+01  -1.19366599e+02   2.31653230e+01\n",
      "    1.07254981e+02  -3.95574346e+01  -4.94776320e+01   5.61483638e+01\n",
      "   -6.90454970e+01  -1.06135158e+02   1.22152595e+02  -1.37599587e+01\n",
      "   -6.89979169e+01   2.97292244e+01  -2.90168804e+01   1.11013754e+02\n",
      "    5.98796575e+01   2.73795953e+01   1.45880999e+02  -5.72348137e+01\n",
      "    3.62557390e+01  -7.21938764e+01  -1.29019957e+02   1.94103115e+01\n",
      "   -1.85908260e+02   1.38087415e+02   1.22649070e+02  -6.51741619e+01\n",
      "   -4.67457775e+01  -3.03981378e+01   9.54515230e+00  -4.92882054e+00\n",
      "    7.74720125e+00  -1.22528217e+01   6.11590199e+01  -8.65106322e+01\n",
      "   -4.65428639e+01  -6.50224752e+00   3.01607918e+01  -3.01731778e+01\n",
      "    7.02344574e+01  -2.54418148e+01   6.77771274e+01  -3.89349146e+01\n",
      "    4.84101263e+01   9.24883807e+01  -1.46637001e+02  -7.93095667e+01]] \n",
      "\n",
      "The first 10 instances:  [9 0] \n",
      "\n",
      "The shape of instance is:  (60000, 128)\n",
      "The shape of label is:  (60000,)\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('train_128.h5','r') as H:\n",
    "    data = np.copy(H['data'])\n",
    "print(\"The first 10 instances: \")\n",
    "print(data[:2],\"\\n\")\n",
    "with h5py.File('train_label.h5','r') as H:\n",
    "    label = np.copy(H['label'])\n",
    "print(\"The first 10 instances: \",label[:2],\"\\n\")\n",
    "\n",
    "print(\"The shape of instance is: \",data.shape)\n",
    "print(\"The shape of label is: \",label.shape)\n",
    "labels = np.array([vectorized_label(tmp) for tmp in label])\n",
    "#print(labels[:10])\n",
    "#data = np.array([np.reshape(tmp, (128, 1)) for tmp in data])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 - Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting.....PCA\n",
      "Considering...K of conponents\n",
      "0.5042902733264708 % variance retained in 2 dimensions\n",
      "0.5042902733264708 % variance retained in 2 dimensions\n",
      "0.6225819057861219 % variance retained in 4 dimensions\n",
      "0.7013422310272825 % variance retained in 6 dimensions\n",
      "0.8046155203256028 % variance retained in 13 dimensions\n",
      "0.8507497292328071 % variance retained in 21 dimensions\n",
      "0.9010996004703893 % variance retained in 36 dimensions\n",
      "0.9509384357849695 % variance retained in 65 dimensions\n",
      "0.9904606017805714 % variance retained in 111 dimensions\n",
      "(48000, 111) (48000, 10)\n",
      "(12000, 111) (12000, 10)\n"
     ]
    }
   ],
   "source": [
    "# PCA\n",
    "temp_training_set = (data-data.mean(0))\n",
    "\n",
    "cov = temp_training_set.T.dot(temp_training_set)/temp_training_set.shape[0]\n",
    "print('Starting.....PCA')\n",
    "U,s,V = np.linalg.svd(cov)\n",
    "#conponent pick\n",
    "print('Considering...K of conponents')\n",
    "valueTotal = np.sum(s)\n",
    "tempTotal = 0;\n",
    "k0=k1=k2=k3=k4=k5=k6=k7=k8=0\n",
    "for i,v in enumerate(s):\n",
    "    tempTotal +=v\n",
    "    if(tempTotal/valueTotal>0.4):\n",
    "        if(k0==0):\n",
    "            k0=i\n",
    "            print('{} % variance retained in {} dimensions'.format(tempTotal/valueTotal, k0+1))\n",
    "    if(tempTotal/valueTotal>0.5):\n",
    "        if(k1==0):\n",
    "            k1=i\n",
    "            print('{} % variance retained in {} dimensions'.format(tempTotal/valueTotal, k1+1))\n",
    "    if(tempTotal/valueTotal>0.6):\n",
    "        if(k2==0):\n",
    "            k2=i\n",
    "            print('{} % variance retained in {} dimensions'.format(tempTotal/valueTotal, k2+1))\n",
    "    if(tempTotal/valueTotal>0.7):\n",
    "        if(k3==0):\n",
    "            k3=i\n",
    "            print('{} % variance retained in {} dimensions'.format(tempTotal/valueTotal, k3+1))\n",
    "    if(tempTotal/valueTotal>0.8):\n",
    "        if(k4==0):\n",
    "            k4=i\n",
    "            print('{} % variance retained in {} dimensions'.format(tempTotal/valueTotal, k4+1))\n",
    "    if(tempTotal/valueTotal>0.85):\n",
    "        if(k5==0):\n",
    "            k5=i\n",
    "            print('{} % variance retained in {} dimensions'.format(tempTotal/valueTotal, k5+1))\n",
    "    if(tempTotal/valueTotal>0.9):\n",
    "        if(k6==0):\n",
    "            k6=i\n",
    "            print('{} % variance retained in {} dimensions'.format(tempTotal/valueTotal, k6+1))\n",
    "    if(tempTotal/valueTotal>0.95):\n",
    "        if(k7==0):\n",
    "            k7=i\n",
    "            print('{} % variance retained in {} dimensions'.format(tempTotal/valueTotal, k7+1))\n",
    "    if(tempTotal/valueTotal>0.99):\n",
    "        if(k8==0):\n",
    "            k8=i\n",
    "            print('{} % variance retained in {} dimensions'.format(tempTotal/valueTotal, k8+1))\n",
    "        else:\n",
    "            break\n",
    "# we choose 95% remained\n",
    "U_reduced = U[:, : k8+1] #as pyhon will not include the last one so add 1 \n",
    "#new_traning_set = U_reduced.T.dot(training_setT)\n",
    "new_traning_set =  temp_training_set.dot(U_reduced)\n",
    "\n",
    "train_data = np.squeeze(new_traning_set[:48000])\n",
    "train_label = np.squeeze(labels[:48000])\n",
    "#train_label = np.expand_dims(train_label,axis=1)\n",
    "\n",
    "print(train_data.shape,train_label.shape)\n",
    "\n",
    "test_data = np.squeeze(new_traning_set[48000:])\n",
    "test_label = np.squeeze(labels[48000:])\n",
    "#test_label = np.expand_dims(test_label,axis=1)\n",
    "print(test_data.shape,test_label.shape)\n",
    "\n",
    "# mean normalization\n",
    "train_data = (train_data-train_data.mean(0))\n",
    "train_data = train_data/np.std(train_data,axis=0)\n",
    "\n",
    "test_data = (test_data-test_data.mean(0))\n",
    "test_data = test_data/np.std(test_data,axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 - Defining the neural network structure ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGDWithWeightnorm(SGD):\n",
    "    def get_updates(self, params, loss):\n",
    "        grads = self.get_gradients(loss, params)\n",
    "        self.updates = []\n",
    "\n",
    "        lr = self.lr\n",
    "        if self.initial_decay > 0:\n",
    "            lr *= (1. / (1. + self.decay * self.iterations))\n",
    "            self.updates .append(K.update_add(self.iterations, 1))\n",
    "\n",
    "        # momentum\n",
    "        shapes = [K.get_variable_shape(p) for p in params]\n",
    "        moments = [K.zeros(shape) for shape in shapes]\n",
    "        self.weights = [self.iterations] + moments\n",
    "        for p, g, m in zip(params, grads, moments):\n",
    "\n",
    "            # if a weight tensor (len > 1) use weight normalized parameterization\n",
    "            ps = K.get_variable_shape(p)\n",
    "            if len(ps) > 1:\n",
    "\n",
    "                # get weight normalization parameters\n",
    "                V, V_norm, V_scaler, g_param, grad_g, grad_V = get_weightnorm_params_and_grads(p, g)\n",
    "\n",
    "                # momentum container for the 'g' parameter\n",
    "                V_scaler_shape = K.get_variable_shape(V_scaler)\n",
    "                m_g = K.zeros(V_scaler_shape)\n",
    "\n",
    "                # update g parameters\n",
    "                v_g = self.momentum * m_g - lr * grad_g  # velocity\n",
    "                self.updates.append(K.update(m_g, v_g))\n",
    "                if self.nesterov:\n",
    "                    new_g_param = g_param + self.momentum * v_g - lr * grad_g\n",
    "                else:\n",
    "                    new_g_param = g_param + v_g\n",
    "\n",
    "                # update V parameters\n",
    "                v_v = self.momentum * m - lr * grad_V  # velocity\n",
    "                self.updates.append(K.update(m, v_v))\n",
    "                if self.nesterov:\n",
    "                    new_V_param = V + self.momentum * v_v - lr * grad_V\n",
    "                else:\n",
    "                    new_V_param = V + v_v\n",
    "\n",
    "                # wn param updates --> W updates\n",
    "                add_weightnorm_param_updates(self.updates, new_V_param, new_g_param, p, V_scaler)\n",
    "\n",
    "            else: # normal SGD with momentum\n",
    "                v = self.momentum * m - lr * g  # velocity\n",
    "                self.updates.append(K.update(m, v))\n",
    "\n",
    "                if self.nesterov:\n",
    "                    new_p = p + self.momentum * v - lr * g\n",
    "                else:\n",
    "                    new_p = p + v\n",
    "\n",
    "                self.updates.append(K.update(p, new_p))\n",
    "        return self.updates\n",
    "def get_weightnorm_params_and_grads(p, g):\n",
    "    ps = K.get_variable_shape(p)\n",
    "\n",
    "    # construct weight scaler: V_scaler = g/||V||\n",
    "    V_scaler_shape = (ps[-1],)  # assumes we're using tensorflow!\n",
    "    V_scaler = K.ones(V_scaler_shape)  # init to ones, so effective parameters don't change\n",
    "\n",
    "    # get V parameters = ||V||/g * W\n",
    "    norm_axes = [i for i in range(len(ps) - 1)]\n",
    "    V = p / tf.reshape(V_scaler, [1] * len(norm_axes) + [-1])\n",
    "\n",
    "    # split V_scaler into ||V|| and g parameters\n",
    "    V_norm = tf.sqrt(tf.reduce_sum(tf.square(V), norm_axes))\n",
    "    g_param = V_scaler * V_norm\n",
    "\n",
    "    # get grad in V,g parameters\n",
    "    grad_g = tf.reduce_sum(g * V, norm_axes) / V_norm\n",
    "    grad_V = tf.reshape(V_scaler, [1] * len(norm_axes) + [-1]) * \\\n",
    "             (g - tf.reshape(grad_g / V_norm, [1] * len(norm_axes) + [-1]) * V)\n",
    "\n",
    "    return V, V_norm, V_scaler, g_param, grad_g, grad_V\n",
    "# data based initialization for a given Keras model\n",
    "def add_weightnorm_param_updates(updates, new_V_param, new_g_param, W, V_scaler):\n",
    "    ps = K.get_variable_shape(new_V_param)\n",
    "    norm_axes = [i for i in range(len(ps) - 1)]\n",
    "\n",
    "    # update W and V_scaler\n",
    "    new_V_norm = tf.sqrt(tf.reduce_sum(tf.square(new_V_param), norm_axes))\n",
    "    new_V_scaler = new_g_param / new_V_norm\n",
    "    new_W = tf.reshape(new_V_scaler, [1] * len(norm_axes) + [-1]) * new_V_param\n",
    "    updates.append(K.update(W, new_W))\n",
    "    updates.append(K.update(V_scaler, new_V_scaler))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(180, input_shape=(111,)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(180))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sgd_wn = SGDWithWeightnorm(lr=0.01, decay=1e-6, momentum=0.5, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',optimizer=sgd_wn,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/200\n",
      "48000/48000 [==============================] - 15s - loss: 1.0348 - acc: 0.6566 - val_loss: 0.4512 - val_acc: 0.8418\n",
      "Epoch 2/200\n",
      "48000/48000 [==============================] - 13s - loss: 0.5934 - acc: 0.7963 - val_loss: 0.4111 - val_acc: 0.8517\n",
      "Epoch 3/200\n",
      "48000/48000 [==============================] - 13s - loss: 0.5256 - acc: 0.8158 - val_loss: 0.3862 - val_acc: 0.8590\n",
      "Epoch 4/200\n",
      "48000/48000 [==============================] - 15s - loss: 0.4906 - acc: 0.8266 - val_loss: 0.3709 - val_acc: 0.8659\n",
      "Epoch 5/200\n",
      "48000/48000 [==============================] - 15s - loss: 0.4676 - acc: 0.8345 - val_loss: 0.3613 - val_acc: 0.8699\n",
      "Epoch 6/200\n",
      "48000/48000 [==============================] - 15s - loss: 0.4526 - acc: 0.8390 - val_loss: 0.3545 - val_acc: 0.8696\n",
      "Epoch 7/200\n",
      "48000/48000 [==============================] - 16s - loss: 0.4349 - acc: 0.8449 - val_loss: 0.3465 - val_acc: 0.8735\n",
      "Epoch 8/200\n",
      "48000/48000 [==============================] - 14s - loss: 0.4288 - acc: 0.8474 - val_loss: 0.3451 - val_acc: 0.8746\n",
      "Epoch 9/200\n",
      "48000/48000 [==============================] - 14s - loss: 0.4179 - acc: 0.8507 - val_loss: 0.3359 - val_acc: 0.8776\n",
      "Epoch 10/200\n",
      "48000/48000 [==============================] - 13s - loss: 0.4086 - acc: 0.8554 - val_loss: 0.3311 - val_acc: 0.8789\n",
      "Epoch 11/200\n",
      "48000/48000 [==============================] - 13s - loss: 0.4017 - acc: 0.8570 - val_loss: 0.3286 - val_acc: 0.8802\n",
      "Epoch 12/200\n",
      "48000/48000 [==============================] - 15s - loss: 0.3928 - acc: 0.8587 - val_loss: 0.3259 - val_acc: 0.8804\n",
      "Epoch 13/200\n",
      "48000/48000 [==============================] - 13s - loss: 0.3852 - acc: 0.8623 - val_loss: 0.3230 - val_acc: 0.8826\n",
      "Epoch 14/200\n",
      "48000/48000 [==============================] - 13s - loss: 0.3788 - acc: 0.8636 - val_loss: 0.3215 - val_acc: 0.8797\n",
      "Epoch 15/200\n",
      "48000/48000 [==============================] - 13s - loss: 0.3749 - acc: 0.8633 - val_loss: 0.3152 - val_acc: 0.8859\n",
      "Epoch 16/200\n",
      "48000/48000 [==============================] - 13s - loss: 0.3727 - acc: 0.8658 - val_loss: 0.3166 - val_acc: 0.8840\n",
      "Epoch 17/200\n",
      "48000/48000 [==============================] - 17s - loss: 0.3671 - acc: 0.8668 - val_loss: 0.3137 - val_acc: 0.8847\n",
      "Epoch 18/200\n",
      "48000/48000 [==============================] - 14s - loss: 0.3626 - acc: 0.8687 - val_loss: 0.3098 - val_acc: 0.8867\n",
      "Epoch 19/200\n",
      "48000/48000 [==============================] - 14s - loss: 0.3614 - acc: 0.8685 - val_loss: 0.3106 - val_acc: 0.8861\n",
      "Epoch 20/200\n",
      "48000/48000 [==============================] - 13s - loss: 0.3578 - acc: 0.8703 - val_loss: 0.3101 - val_acc: 0.8866\n",
      "Epoch 21/200\n",
      "48000/48000 [==============================] - 13s - loss: 0.3543 - acc: 0.8706 - val_loss: 0.3054 - val_acc: 0.8869\n",
      "Epoch 22/200\n",
      "48000/48000 [==============================] - 14s - loss: 0.3468 - acc: 0.8735 - val_loss: 0.3068 - val_acc: 0.8877\n",
      "Epoch 23/200\n",
      "48000/48000 [==============================] - 14s - loss: 0.3488 - acc: 0.8745 - val_loss: 0.3026 - val_acc: 0.8894\n",
      "Epoch 24/200\n",
      "48000/48000 [==============================] - 14s - loss: 0.3476 - acc: 0.8739 - val_loss: 0.3036 - val_acc: 0.8889\n",
      "Epoch 25/200\n",
      "48000/48000 [==============================] - 14s - loss: 0.3369 - acc: 0.8776 - val_loss: 0.2996 - val_acc: 0.8897\n",
      "Epoch 26/200\n",
      "48000/48000 [==============================] - 15s - loss: 0.3413 - acc: 0.8752 - val_loss: 0.3013 - val_acc: 0.8902\n",
      "Epoch 27/200\n",
      "48000/48000 [==============================] - 14s - loss: 0.3365 - acc: 0.8770 - val_loss: 0.2987 - val_acc: 0.8902\n",
      "Epoch 28/200\n",
      "48000/48000 [==============================] - 14s - loss: 0.3316 - acc: 0.8783 - val_loss: 0.2966 - val_acc: 0.8905\n",
      "Epoch 29/200\n",
      "48000/48000 [==============================] - 14s - loss: 0.3298 - acc: 0.8777 - val_loss: 0.2947 - val_acc: 0.8914\n",
      "Epoch 30/200\n",
      "48000/48000 [==============================] - 14s - loss: 0.3256 - acc: 0.8799 - val_loss: 0.2958 - val_acc: 0.8912\n",
      "Epoch 31/200\n",
      "48000/48000 [==============================] - 15s - loss: 0.3282 - acc: 0.8792 - val_loss: 0.2970 - val_acc: 0.8899\n",
      "Epoch 32/200\n",
      "48000/48000 [==============================] - 15s - loss: 0.3242 - acc: 0.8822 - val_loss: 0.2958 - val_acc: 0.8910\n",
      "Epoch 33/200\n",
      "48000/48000 [==============================] - 15s - loss: 0.3184 - acc: 0.8826 - val_loss: 0.2929 - val_acc: 0.8913\n",
      "Epoch 34/200\n",
      "48000/48000 [==============================] - 14s - loss: 0.3212 - acc: 0.8810 - val_loss: 0.2911 - val_acc: 0.8921\n",
      "Epoch 35/200\n",
      "48000/48000 [==============================] - 14s - loss: 0.3180 - acc: 0.8844 - val_loss: 0.2914 - val_acc: 0.8932\n",
      "Epoch 36/200\n",
      "48000/48000 [==============================] - 13s - loss: 0.3144 - acc: 0.8854 - val_loss: 0.2915 - val_acc: 0.8934\n",
      "Epoch 37/200\n",
      "48000/48000 [==============================] - 13s - loss: 0.3166 - acc: 0.8835 - val_loss: 0.2897 - val_acc: 0.8925\n",
      "Epoch 38/200\n",
      "48000/48000 [==============================] - 14s - loss: 0.3136 - acc: 0.8850 - val_loss: 0.2899 - val_acc: 0.8942\n",
      "Epoch 39/200\n",
      "48000/48000 [==============================] - 13s - loss: 0.3138 - acc: 0.8860 - val_loss: 0.2903 - val_acc: 0.8932\n",
      "Epoch 40/200\n",
      "48000/48000 [==============================] - 13s - loss: 0.3139 - acc: 0.8846 - val_loss: 0.2876 - val_acc: 0.8944\n",
      "Epoch 41/200\n",
      "48000/48000 [==============================] - 12s - loss: 0.3093 - acc: 0.8858 - val_loss: 0.2892 - val_acc: 0.8947\n",
      "Epoch 42/200\n",
      "48000/48000 [==============================] - 12s - loss: 0.3043 - acc: 0.8882 - val_loss: 0.2882 - val_acc: 0.8952\n",
      "Epoch 43/200\n",
      "48000/48000 [==============================] - 13s - loss: 0.3077 - acc: 0.8859 - val_loss: 0.2879 - val_acc: 0.8933\n",
      "Epoch 44/200\n",
      "48000/48000 [==============================] - 13s - loss: 0.3044 - acc: 0.8880 - val_loss: 0.2877 - val_acc: 0.8939\n",
      "Epoch 45/200\n",
      "48000/48000 [==============================] - 13s - loss: 0.3077 - acc: 0.8885 - val_loss: 0.2889 - val_acc: 0.8946\n",
      "Epoch 46/200\n",
      "48000/48000 [==============================] - 15s - loss: 0.3034 - acc: 0.8886 - val_loss: 0.2862 - val_acc: 0.8959\n",
      "Epoch 47/200\n",
      "48000/48000 [==============================] - 14s - loss: 0.3024 - acc: 0.8889 - val_loss: 0.2865 - val_acc: 0.8953\n",
      "Epoch 48/200\n",
      "48000/48000 [==============================] - 14s - loss: 0.2992 - acc: 0.8896 - val_loss: 0.2849 - val_acc: 0.8948\n",
      "Epoch 49/200\n",
      "48000/48000 [==============================] - 14s - loss: 0.2968 - acc: 0.8910 - val_loss: 0.2855 - val_acc: 0.8953\n",
      "Epoch 50/200\n",
      "48000/48000 [==============================] - 14s - loss: 0.2954 - acc: 0.8903 - val_loss: 0.2842 - val_acc: 0.8967\n",
      "Epoch 51/200\n",
      "48000/48000 [==============================] - 13s - loss: 0.2908 - acc: 0.8939 - val_loss: 0.2845 - val_acc: 0.8961\n",
      "Epoch 52/200\n",
      "48000/48000 [==============================] - 14s - loss: 0.2945 - acc: 0.8926 - val_loss: 0.2825 - val_acc: 0.8972\n",
      "Epoch 53/200\n",
      "48000/48000 [==============================] - 14s - loss: 0.2914 - acc: 0.8927 - val_loss: 0.2832 - val_acc: 0.8970\n",
      "Epoch 54/200\n",
      "48000/48000 [==============================] - 15s - loss: 0.2908 - acc: 0.8922 - val_loss: 0.2831 - val_acc: 0.8961\n",
      "Epoch 55/200\n",
      "48000/48000 [==============================] - 13s - loss: 0.2932 - acc: 0.8920 - val_loss: 0.2836 - val_acc: 0.8960\n",
      "Epoch 56/200\n",
      "48000/48000 [==============================] - 13s - loss: 0.2917 - acc: 0.8934 - val_loss: 0.2832 - val_acc: 0.8962\n",
      "Epoch 57/200\n",
      "48000/48000 [==============================] - 13s - loss: 0.2900 - acc: 0.8925 - val_loss: 0.2838 - val_acc: 0.8956\n",
      "Epoch 58/200\n",
      "48000/48000 [==============================] - 13s - loss: 0.2882 - acc: 0.8939 - val_loss: 0.2817 - val_acc: 0.8980\n",
      "Epoch 59/200\n",
      "48000/48000 [==============================] - 13s - loss: 0.2861 - acc: 0.8934 - val_loss: 0.2820 - val_acc: 0.8949\n",
      "Epoch 60/200\n",
      "48000/48000 [==============================] - 13s - loss: 0.2850 - acc: 0.8954 - val_loss: 0.2815 - val_acc: 0.8970\n",
      "Epoch 61/200\n",
      "48000/48000 [==============================] - 13s - loss: 0.2847 - acc: 0.8955 - val_loss: 0.2837 - val_acc: 0.8972\n",
      "Epoch 62/200\n",
      "48000/48000 [==============================] - 13s - loss: 0.2872 - acc: 0.8945 - val_loss: 0.2815 - val_acc: 0.8979\n",
      "Epoch 63/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000/48000 [==============================] - 13s - loss: 0.2820 - acc: 0.8955 - val_loss: 0.2816 - val_acc: 0.8970\n",
      "Epoch 64/200\n",
      "48000/48000 [==============================] - 13s - loss: 0.2830 - acc: 0.8961 - val_loss: 0.2834 - val_acc: 0.8986\n",
      "Epoch 65/200\n",
      "48000/48000 [==============================] - 13s - loss: 0.2817 - acc: 0.8954 - val_loss: 0.2827 - val_acc: 0.8974\n",
      "Epoch 66/200\n",
      "48000/48000 [==============================] - 13s - loss: 0.2814 - acc: 0.8975 - val_loss: 0.2815 - val_acc: 0.8982\n",
      "Epoch 67/200\n",
      "48000/48000 [==============================] - 13s - loss: 0.2788 - acc: 0.8964 - val_loss: 0.2822 - val_acc: 0.8971\n",
      "Epoch 68/200\n",
      "48000/48000 [==============================] - 14s - loss: 0.2810 - acc: 0.8958 - val_loss: 0.2820 - val_acc: 0.8967\n",
      "Epoch 69/200\n",
      "48000/48000 [==============================] - 15s - loss: 0.2773 - acc: 0.8970 - val_loss: 0.2820 - val_acc: 0.8977\n",
      "Epoch 70/200\n",
      "48000/48000 [==============================] - 13s - loss: 0.2799 - acc: 0.8980 - val_loss: 0.2799 - val_acc: 0.8977\n",
      "Epoch 71/200\n",
      "48000/48000 [==============================] - 15s - loss: 0.2768 - acc: 0.8982 - val_loss: 0.2809 - val_acc: 0.8971\n",
      "Epoch 72/200\n",
      "48000/48000 [==============================] - 14s - loss: 0.2776 - acc: 0.8978 - val_loss: 0.2773 - val_acc: 0.8977\n",
      "Epoch 73/200\n",
      "48000/48000 [==============================] - 14s - loss: 0.2746 - acc: 0.8990 - val_loss: 0.2798 - val_acc: 0.8972\n",
      "Epoch 74/200\n",
      "48000/48000 [==============================] - 14s - loss: 0.2749 - acc: 0.8970 - val_loss: 0.2808 - val_acc: 0.8977\n",
      "Epoch 75/200\n",
      "48000/48000 [==============================] - 14s - loss: 0.2759 - acc: 0.8980 - val_loss: 0.2789 - val_acc: 0.8984\n",
      "Epoch 76/200\n",
      "48000/48000 [==============================] - 13s - loss: 0.2740 - acc: 0.8980 - val_loss: 0.2797 - val_acc: 0.8992\n",
      "Epoch 77/200\n",
      "48000/48000 [==============================] - 14s - loss: 0.2752 - acc: 0.8979 - val_loss: 0.2822 - val_acc: 0.8977\n",
      "Epoch 78/200\n",
      "48000/48000 [==============================] - 14s - loss: 0.2744 - acc: 0.8978 - val_loss: 0.2820 - val_acc: 0.8976\n",
      "Epoch 79/200\n",
      "48000/48000 [==============================] - 13s - loss: 0.2753 - acc: 0.8975 - val_loss: 0.2821 - val_acc: 0.8982\n",
      "Epoch 80/200\n",
      "48000/48000 [==============================] - 14s - loss: 0.2710 - acc: 0.8998 - val_loss: 0.2834 - val_acc: 0.8983\n",
      "Epoch 81/200\n",
      "48000/48000 [==============================] - 13s - loss: 0.2732 - acc: 0.8972 - val_loss: 0.2813 - val_acc: 0.8987\n",
      "Epoch 82/200\n",
      "48000/48000 [==============================] - 13s - loss: 0.2730 - acc: 0.8999 - val_loss: 0.2808 - val_acc: 0.8986\n",
      "Epoch 83/200\n",
      "48000/48000 [==============================] - 13s - loss: 0.2691 - acc: 0.9016 - val_loss: 0.2804 - val_acc: 0.8984\n",
      "Epoch 84/200\n",
      "48000/48000 [==============================] - 13s - loss: 0.2681 - acc: 0.8999 - val_loss: 0.2807 - val_acc: 0.8986\n",
      "Epoch 85/200\n",
      "48000/48000 [==============================] - 13s - loss: 0.2678 - acc: 0.8999 - val_loss: 0.2824 - val_acc: 0.8993\n",
      "Epoch 86/200\n",
      "48000/48000 [==============================] - 13s - loss: 0.2700 - acc: 0.8991 - val_loss: 0.2797 - val_acc: 0.8983\n",
      "Epoch 87/200\n",
      "48000/48000 [==============================] - 14s - loss: 0.2670 - acc: 0.9008 - val_loss: 0.2830 - val_acc: 0.8978\n",
      "Epoch 88/200\n",
      "48000/48000 [==============================] - 14s - loss: 0.2636 - acc: 0.9010 - val_loss: 0.2798 - val_acc: 0.9004\n",
      "Epoch 89/200\n",
      "48000/48000 [==============================] - 13s - loss: 0.2685 - acc: 0.9006 - val_loss: 0.2807 - val_acc: 0.8981\n",
      "Epoch 90/200\n",
      "48000/48000 [==============================] - 13s - loss: 0.2644 - acc: 0.9024 - val_loss: 0.2782 - val_acc: 0.9006\n",
      "Epoch 91/200\n",
      "48000/48000 [==============================] - 13s - loss: 0.2651 - acc: 0.9014 - val_loss: 0.2780 - val_acc: 0.8990\n",
      "Epoch 92/200\n",
      "48000/48000 [==============================] - 14s - loss: 0.2647 - acc: 0.9014 - val_loss: 0.2790 - val_acc: 0.9007\n",
      "Epoch 93/200\n",
      "48000/48000 [==============================] - 14s - loss: 0.2664 - acc: 0.9010 - val_loss: 0.2797 - val_acc: 0.8992\n",
      "Epoch 94/200\n",
      "48000/48000 [==============================] - 13s - loss: 0.2609 - acc: 0.9030 - val_loss: 0.2788 - val_acc: 0.8984\n",
      "Epoch 95/200\n",
      "48000/48000 [==============================] - 13s - loss: 0.2645 - acc: 0.9025 - val_loss: 0.2783 - val_acc: 0.8997\n",
      "Epoch 96/200\n",
      "48000/48000 [==============================] - 13s - loss: 0.2627 - acc: 0.9026 - val_loss: 0.2812 - val_acc: 0.8992\n",
      "Epoch 97/200\n",
      "48000/48000 [==============================] - 13s - loss: 0.2629 - acc: 0.9025 - val_loss: 0.2795 - val_acc: 0.8988\n",
      "Epoch 98/200\n",
      "48000/48000 [==============================] - 13s - loss: 0.2657 - acc: 0.9008 - val_loss: 0.2802 - val_acc: 0.8987\n",
      "Epoch 99/200\n",
      "48000/48000 [==============================] - 13s - loss: 0.2637 - acc: 0.9021 - val_loss: 0.2807 - val_acc: 0.9001\n",
      "Epoch 100/200\n",
      "48000/48000 [==============================] - 13s - loss: 0.2653 - acc: 0.9033 - val_loss: 0.2797 - val_acc: 0.8989\n",
      "Epoch 101/200\n",
      "48000/48000 [==============================] - 13s - loss: 0.2613 - acc: 0.9024 - val_loss: 0.2790 - val_acc: 0.8987\n",
      "Epoch 102/200\n",
      "48000/48000 [==============================] - 13s - loss: 0.2615 - acc: 0.9032 - val_loss: 0.2812 - val_acc: 0.8986\n",
      "Epoch 103/200\n",
      "48000/48000 [==============================] - 14s - loss: 0.2586 - acc: 0.9036 - val_loss: 0.2799 - val_acc: 0.8997\n",
      "Epoch 104/200\n",
      "48000/48000 [==============================] - 13s - loss: 0.2610 - acc: 0.9008 - val_loss: 0.2801 - val_acc: 0.8987\n",
      "Epoch 105/200\n",
      "48000/48000 [==============================] - 13s - loss: 0.2623 - acc: 0.9025 - val_loss: 0.2794 - val_acc: 0.8980\n",
      "Epoch 106/200\n",
      "48000/48000 [==============================] - 13s - loss: 0.2585 - acc: 0.9042 - val_loss: 0.2787 - val_acc: 0.8991\n",
      "Epoch 107/200\n",
      "48000/48000 [==============================] - 13s - loss: 0.2568 - acc: 0.9054 - val_loss: 0.2821 - val_acc: 0.8999\n",
      "Epoch 108/200\n",
      "48000/48000 [==============================] - 13s - loss: 0.2607 - acc: 0.9032 - val_loss: 0.2802 - val_acc: 0.8998\n",
      "Epoch 109/200\n",
      "48000/48000 [==============================] - 13s - loss: 0.2606 - acc: 0.9024 - val_loss: 0.2806 - val_acc: 0.9001\n",
      "Epoch 110/200\n",
      "48000/48000 [==============================] - 13s - loss: 0.2584 - acc: 0.9030 - val_loss: 0.2820 - val_acc: 0.8993\n",
      "Epoch 111/200\n",
      "48000/48000 [==============================] - 13s - loss: 0.2566 - acc: 0.9044 - val_loss: 0.2832 - val_acc: 0.8976\n",
      "Epoch 112/200\n",
      "48000/48000 [==============================] - 13s - loss: 0.2603 - acc: 0.9034 - val_loss: 0.2858 - val_acc: 0.8970\n",
      "Epoch 113/200\n",
      "48000/48000 [==============================] - 13s - loss: 0.2553 - acc: 0.9055 - val_loss: 0.2818 - val_acc: 0.8997\n",
      "Epoch 114/200\n",
      "48000/48000 [==============================] - 13s - loss: 0.2562 - acc: 0.9049 - val_loss: 0.2824 - val_acc: 0.8990\n",
      "Epoch 115/200\n",
      "48000/48000 [==============================] - 13s - loss: 0.2516 - acc: 0.9070 - val_loss: 0.2843 - val_acc: 0.8988\n",
      "Epoch 116/200\n",
      "48000/48000 [==============================] - 13s - loss: 0.2518 - acc: 0.9056 - val_loss: 0.2829 - val_acc: 0.8992\n",
      "Epoch 117/200\n",
      "48000/48000 [==============================] - 13s - loss: 0.2558 - acc: 0.9051 - val_loss: 0.2833 - val_acc: 0.8998\n",
      "Epoch 118/200\n",
      "48000/48000 [==============================] - 13s - loss: 0.2545 - acc: 0.9055 - val_loss: 0.2815 - val_acc: 0.8991\n",
      "Epoch 119/200\n",
      "48000/48000 [==============================] - 13s - loss: 0.2563 - acc: 0.9049 - val_loss: 0.2830 - val_acc: 0.8985\n",
      "Epoch 120/200\n",
      "48000/48000 [==============================] - 13s - loss: 0.2506 - acc: 0.9058 - val_loss: 0.2820 - val_acc: 0.8996\n",
      "Epoch 121/200\n",
      "48000/48000 [==============================] - 14s - loss: 0.2569 - acc: 0.9040 - val_loss: 0.2823 - val_acc: 0.8993\n",
      "Epoch 122/200\n",
      "48000/48000 [==============================] - 14s - loss: 0.2503 - acc: 0.9066 - val_loss: 0.2836 - val_acc: 0.8982\n",
      "Epoch 123/200\n",
      " 8760/48000 [====>.........................] - ETA: 10s - loss: 0.2601 - acc: 0.9040"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-121-e702f952ae39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m               \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m               \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m               shuffle=True)\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    865\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 867\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1593\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1594\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1595\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1597\u001b[0m     def evaluate(self, x, y,\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1180\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1182\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1183\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2268\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2269\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2270\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2271\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(train_data, train_label,\n",
    "              batch_size=30,\n",
    "              epochs=200,\n",
    "              validation_data=(test_data, test_label),\n",
    "              shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "wRuwL",
   "launcher_item_id": "NI888"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
